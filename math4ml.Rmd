---
title: CS5014 Machine Learning
subtitle: Lecture 2 Maths background review 
author: "Lei Fang"
date: Spring 2021
output: 
  beamer_presentation:
    keep_tex: true
#    toc: true
#    slide_level: 3
    includes:
      in_header: 
        - sta-beamer-header-simple.tex
        - l2.tex
        # - title-page.tex
#      after_body: ~/Dropbox/teaching/table-of-contents.txt
bibliography: "ref.bib"
---


```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(webgl = hook_webgl,
                      echo = FALSE,
                      fig.align = 'center',
                      message =FALSE,
                      warning = FALSE
                      )
```

```{r, include = FALSE}
library(tidyverse)
library(reticulate)
# py_config()
```
# Introduction

## So why this review session ?
Maths is useful 

- rigorous and concise way of communicating results 
- help us understand why *and* why not models work
- be able to derive your own model and algorithms!
  
\pause 
\bigskip 

Refresher on essential concepts

- only a refresher; we expect you have learnt them
  - don't expect to know everything after this
- not complete and not rigorous 

\pause
\bigskip A chance for self-assessment 
  
- identify rusty area
- do self studies afterwards
- maths learning should be never-ending :-)

## Mathematics for machine learning 

Linear algebra

- leap forward from 1-d to $N$-dimensional
- number line axis to a plane and hyper-space
<!-- e.g. $a + b = c$  -->
<!--   - $a,b,c$are scalars: $1+2 = 3$ -->
<!--   - $a,b,c$are vectors: $\begin{bmatrix} -->
<!--            1 \\ -->
<!--            2  -->
<!--          \end{bmatrix} + \begin{bmatrix} -->
<!--            2 \\ -->
<!--            1  -->
<!--          \end{bmatrix} = \begin{bmatrix} -->
<!--            3 \\ -->
<!--            3  -->
<!--          \end{bmatrix}$ -->



\bigskip Calculus 

- continuous (real-valued) functions by approximation, (say *polynomial*)
  - $y= sin(x)$ is approximated by $y=x$ when $x \approx 0$ 
- useful for optimisation 




\bigskip Probability theory and statistics 

- study of uncertainty: uncertainty is the norm
  - e.g. rain tomorrow? blood pressure measurement (reading error)? 
- how to generalise your results 
  - from one sample to the universe: training error $\rightarrow$ testing?



## Useful textbook and references (read the italic entries!)

**Linear algebra**

  - [*Learning from Data Supplementary Mathematics (Vector and Linear Algebra) by David Barber*; https://api.semanticscholar.org/CorpusID:18857001](https://api.semanticscholar.org/CorpusID:18857001)
  - [*Chapter 2 of Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville* https://www.deeplearningbook.org/contents/linear_algebra.html](https://www.deeplearningbook.org/contents/linear_algebra.html)
  - [Introduction to Linear Algebra by Gilbert Strang; http://math.mit.edu/~gs/linearalgebra/](http://math.mit.edu/~gs/linearalgebra/)
  - [The Matrix Cookbook by Kaare Brandt Petersen, Michael Syskind Pedersen; https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html)
    - useful as a reference manual
    
##    
    
**Probability theory**

  - [*Chapter 2.1-2.3 Information Theory, Inference, and Learning Algorithms by David J.C. MacKay*  http://www.inference.org.uk/itprnn/book.pdf](http://www.inference.org.uk/itprnn/book.pdf)
  - [*Chapter 3.1-3.9 of Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville* https://www.deeplearningbook.org/contents/prob.html](https://www.deeplearningbook.org/contents/prob.html)
  - Introduction to Probability Models by Sheldon Ross
    - chapter 1; chapter 2.1-2.5, 2.8; chapter 3.1-3.5
    


**Calculus**
  
  - Use your book of choice; read multivariate calculus part as well
  - [Appendix of Bayesian Reasoning and Machine Learning by David Barber http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf ](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf)
  
# Linear Algebra
## Linear algebra: basic concepts

- vectors
\bigskip
- norms and projection
\bigskip
- linear independence, span, subspace
\bigskip
- matrices, linear transformation
\bigskip
- rank, determinant, trace 


## Vector 

A vector is a collection of $n$ salars

- $\vv{a} \in R^n$, default option is column vector i.e. $n\times 1$
- represents a **displacement**  in $R^n$
- e.g. $\vv{a} = \begin{bmatrix}
           2 \\
           1 
         \end{bmatrix}, \textcolor{red}{\vv{b} = \begin{bmatrix}
           1 \\
           2 
         \end{bmatrix}}$ (or $\vv{a} = [2, 1]^T$ to save space)
\bigskip   
```{r, figures-side, fig.show="hold", out.width="49%"}
library(matlib)
#setting up the plot
xlim <- c(-1,4)
ylim <- c(-1,4)
par(mar=c(3,3,3,0.1)+.1)
plot(xlim, ylim, type="n", xlab="", ylab="", cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)
grid()
# define some vectors
a=c(2,1)
b=c(1,2)
# plot the vectors
vectors(b, labels="b", pos.lab=4, cex.lab = 3, col="red")
vectors(a, labels="a", pos.lab=4, cex.lab = 3)

# vec <- rbind(diag(3), c(1,1,1))
# rownames(vec) <- c("X", "Y", "Z", "J")
# open3d()
# vectors3d(vec, color=c(rep("black",3), "red"), lwd=2)
# # draw the XZ plane, whose equation is Y=0
# planes3d(0, 0, 1, 0, col="gray", alpha=0.2)
# vectors3d(c(1,1,0), col="green", lwd=2)
# plot(xlim, ylim, type="n", xlab="", ylab="", cex.lab=2.5, cex.axis=2.5, cex.main=3.5, cex.sub=2.5, main = "all the b vectors are the same")
# grid()
# vectors(b, labels="b", pos.lab=4, cex.lab = 3, col="red")
# displacement = c(2,0)
# vectors(b+displacement, labels="b", pos.lab=4, cex.lab = 3, origin = displacement, col="red")
# displacement = c(1,0)
# vectors(b+displacement, labels="b", pos.lab=4, cex.lab = 3, origin = displacement, col="red")
# displacement = c(2,-1)
# vectors(b+displacement, labels="b", pos.lab=4, cex.lab = 3, origin = displacement, col="red")
# displacement = c(0,1)
# vectors(b+displacement, labels="b", pos.lab=4, cex.lab = 3, origin = displacement, col="red")
# displacement = c(0,2)
# vectors(b+displacement, labels="b", pos.lab=4, cex.lab = 3, origin = displacement, col="red")
# vector a+b starting from a is equal to b.
#vectors(a+b, labels="b", pos.lab=4, frac.lab=.5, origin=a, col="green")

```

##
\bigskip

Some 3-d vectors  $\vv{c} = \begin{bmatrix}
           2 \\
           2 \\
           2
         \end{bmatrix}, \vv{d} = \begin{bmatrix}
           2 \\
           0 \\
           0
         \end{bmatrix}, \vv{e} = \begin{bmatrix}
           3 \\
           2 \\
           1
         \end{bmatrix}$ 
\bigskip   
```{r, out.width="55%"}
library("plot3D")
x0 <- c(0,0,0)
y0 <- c(0,0,0)
z0 <- c(0,0,0)
x1 <- c(2,2,3)
y1 <- c(2,0,2)
z1 <- c(2,0,1)
cols <- c("#1B9E77", "#D95F02", "#7570B3", "#E7298A")

arrows3D(x0, y0, z0, x1, y1, z1, col = cols,
         lwd = 2, d = 3, 
         main = "", bty ="g", ticktype = "detailed", xlim= c(-0.5, 3), ylim= c(0, 2), zlim= c(0, 2))
# Add starting point of arrow
# points3D(x0, add = TRUE, col="darkred", 
#           colkey = FALSE, pch = 19, cex = 1)
# Add labels to the arrows
text3D(x1, y1, z1, c("c", "d", "e"),
        col = cols, add=TRUE, colkey = FALSE)
```
## Vector addition 


$$\vv{a}+\vv{b} = \begin{bmatrix}
           a_1 \\
           a_2 \\
           \vdots\\
           a_d
         \end{bmatrix} +  \begin{bmatrix}
           b_1 \\
           b_2 \\
           \vdots\\
           b_d
         \end{bmatrix} = \begin{bmatrix}
           a_1+b_1 \\
           a_2+b_2 \\
           \vdots\\
           a_d+b_d
         \end{bmatrix}$$
         
  - generalisation from scalar addition; remember 2+1 on a number axis ? 
  - parallelogram rule
\bigskip
```{r, out.width="50%"}
#setting up the plot

xlim <- c(-0.1,3.5)
ylim <- c(-0.1,3.5)
par(mar=c(3,3,3,0.1)+.1)
plot(xlim, ylim, type="n", xlab="", ylab="", cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)

grid()
# define some vectors
a=c(2,1)
b=c(1,2)
# plot the vectors

vectors(b, labels="b", pos.lab=4, cex.lab = 3, col="green")
vectors(a, labels="a", pos.lab=4, cex.lab = 3)
vectors(a+b, labels="a+b", pos.lab=4, cex.lab=3, col="red")
# vector a+b starting from a is equal to b.
vectors(a+b, labels="b", pos.lab=4,  cex.lab=2, frac.lab = 0.5, origin=a, col="green", lty=2)

vectors(a+b, pos.lab=4, frac.lab=.5, origin=b, col="black", lty=2)

```



## Vector scaling/multiplication

$$k\cdot \vv{a} = k \cdot \begin{bmatrix}
           a_1 \\
           a_2 \\
           \vdots\\
           a_d
         \end{bmatrix} = \begin{bmatrix}
           k\times a_1 \\
           k\times a_2 \\
           \vdots\\
           k\times a_d
         \end{bmatrix}, k\in R \text{ or a scalar}$$
     
  - geometrically, scaling means shrinking or streching a vector 
    - the direction does not change but length changes
  - arithmetically, $n\cdot \vv{a} = \vv{a}+\ldots+ \vv{a}=\sum_n \vv{a}$
  - $0\cdot \vv{a} = \vv{0}$
```{r, out.width="45%"}
#setting up the plot

xlim <- c(0,8)
ylim <- c(0,4)
par(mar=c(3,3,3,0.1)+.1)
plot(xlim, ylim, type="n", xlab="", ylab="", cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)

grid()
# define some vectors
a=c(2,1)
# plot the vectors


vectors(a, labels="a", pos.lab=4, cex.lab = 3)
vectors(3*a, labels="3*a", pos.lab=4, cex.lab=3, col="red")
vectors(0.5*a, labels="0.5*a", pos.lab=3, frac.lab = 0.5, cex.lab=2.5, col="blue")
# vector a+b starting from a is equal to b.

```


## Inner product 

$$\vv{a}^T \vv{b} = [a_1, a_2\ldots, a_d] \cdot \begin{bmatrix}
            b_1 \\
            b_2 \\
           \vdots\\
            b_d
         \end{bmatrix} = \sum_{i=1}^d a_i\times b_i$$
         
  - $\vv{a}^T \vv{b} = \vv{b}^T\vv{a}$ and the result is a scalar
  - $\vv{a}^T(\vv{b}+\vv{c}) = \vv{a}^T\vv{b}+ \vv{a}^T\vv{c}$
  - $(k\vv{a})^T\vv{b} = \vv{a}^T(k\vv{b})= k(\vv{a}^T\vv{b})$
  - $\vv{a}^T \vv{a} = \sum_{i=1}^d a_i^2$ is squared Euclidean distance between $\vv{a}$ and $\vv{0}$
  - $\vv{a}^T \vv{a} \geq 0$ and $\vv{a}=\vv{0}$ if and only if $\vv{a}^T\vv{a} =0$ 

## Inner product and projection

Another interpretation: $$\vv{a}^T \vv{b} = \|\vv{a}\| \|\vv{b}\|cos \theta$$
```{r, out.width="45%"}
a <- c(3,1)
b <- c(1,3)
sum <- a+b

xlim <- c(0,4)
ylim <- c(0,3.5)
# proper geometry requires asp=1
plot( xlim, ylim, type="n",  asp = 1, xlab="", ylab="", cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)
# abline(v=0, h=0, col="gray")
grid()
vectors(rbind(a,b), col=c("red", "blue"), cex.lab=c(2.5, 2.5))
# show the opposing sides of the parallelogram
# vectors(sum, origin=u, col="red", lty=2)
# vectors(sum, origin=v, col="blue", lty=2)

# projection of vectors
vectors(Proj(b,a), labels="P(b,a)", frac.lab = 0.5, lwd=3, cex.lab = 2.5)
vectors(b, origin=Proj(b,a))
corner(c(0,0), Proj(b,a), b, d=0.3, col="black")
arc(Proj(b,a), c(0,0), b, d=.5, col="red")

text(0.6, 0.6, labels = expression(theta), cex=2)
```

  - $\theta$ is the angle between $\vv{a}, \vv{b}$
    - $\vv{a}^T\vv{b}=0$ if and only if $\vv{a} \perp \vv{b}$
  <!-- - $\|\vv{a}\|cos \theta$ is the projected length of $\vv{a}$ on $\vv{b}$ -->
  - $\|\vv{b}\|cos \theta = \|P(\vv{b}, \vv{a})\|$ is the projected length of $\vv{b}$ on $\vv{a}$
  - $P(\vv{b}, \vv{a})$ denotes the projected vector of $\vv{b}$ to $\vv{a}$
   $$P(\vv{b}, \vv{a}) = \|\vv{b}\|cos \theta * \frac{\vv{a}}{\|\vv{a}\|} =  \frac{\vv{a}^T\vv{b}}{\vv{a}^T\vv{a}}  \vv{a}$$


## Matrix

A rectangular array of real numbers $A\in R^{m\times n}$
$$\vv{A} = \begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n} \\
\vdots & \vdots & & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{bmatrix}  = \begin{bmatrix}
\vert & \vert &  & \vert\\
\vv{a}_1 & \vv{a}_2 & \ldots & \vv{a}_{n} \\
\vert & \vert & & \vert 
\end{bmatrix} = \begin{bmatrix}
\text{---} & \vv{\tilde{a}}_1 & \text{---}\\
\vdots & \vdots & \vdots  \\
\text{---} & \vv{\tilde{a}}_m & \text{---}
\end{bmatrix} $$

  - can be viewed as a collection of n column vectors $\vv{A} = [\vv{a}_1, \vv{a}_2, \ldots, \vv{a}_n]$;
  - or row vectors $\vv{A} = [\vv{\tilde{a}}_1^T, \vv{\tilde{a}}_2^T, \ldots, \vv{\tilde{a}}_m^T]^T$
  - sometimes written as $\vv{A} = (a_{ij})$ $i= 1,\ldots m, j= 1,\ldots, n$
  
  
## Matrix operations

- addition: $\vv{A} +\vv{B} =\vv{C}=(c_{ij})$ where $c_{ij} = a_{ij} +b_{ij}$
- scaling: $k\vv{A}= \vv{C}$ where $c_{ij} =k* a_{ij}$ 
- transpose: $\vv{A}^T = \vv{C}$ where $c_{ij} = a_{ji}$
- multiplication: Let $\vv{A}\in R^{m\times s}, \vv{B} \in R^{s\times n}$ $$\vv{AB} = \vv{C}, \vv{C} \in R^{m\times n}$$  where $$c_{ij} = \sum_{k=1}^s a_{ik}b_{jk}$$ or $c_{ij} = \vv{\tilde{a}}_i^T\vv{b}_j$
  - $\vv{A}(\vv{BC}) =(\vv{AB})\vv{C}$
  - $\vv{AB} \neq \vv{BA}$
  - $(\vv{AB})^T = \vv{B}^T \vv{A}^T$
  - $\vv{I}$ identity matrix: $\vv{I}\vv{A} = \vv{A}$ or $\vv{AI}=\vv{A}$
- inverse (only square matrix): $\vv{A}^{-1} \vv{A}= \vv{AA}^{-1} =\vv{I}$

## Examples 

```{r}
A <- matrix(c(2,3,6,4,1,1), 3, byrow = T)
B <- matrix(c(5,1,2,1,2,2), 2)
C= A%*% B
write_matex2 <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(round(x,2), collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}
```
$$
`r write_matex2(A)` + `r write_matex2(B)`  = ?
$$
\pause it is not allowed as the dimensions do not match


 
$$
`r write_matex2(A)`^T   = `r write_matex2(t(A))`
$$

## Example



$$
`r write_matex2(A)` \times `r write_matex2(B)`  =  \begin{bmatrix} \tcbhighmath[boxrule=2pt,arc=1pt,colback=blue!10!white,colframe=blue,
  drop fuzzy shadow=red]{ 2\times 5+3\times 1 }& c_{12} & c_{13}\\
c_{21}& c_{22} & c_{23}\\
c_{31}& c_{32} & c_{33}
\end{bmatrix}
$$
$$
`r write_matex2(A)` \times `r write_matex2(B)`  =  `r write_matex2(C)`
$$

$$
`r write_matex2(B)` \times `r write_matex2(A)`  =  `r write_matex2(B%*%A)`
$$

## Example
The inverse of $\vv{I} = \begin{bmatrix} 1 &0 \\ 0 & 1 \end{bmatrix}$ is itself $\vv{I}^{-1} =\vv{I}$

\bigskip
The inverse of $\vv{A} = \begin{bmatrix} 3& 0 \\ 0 & 5\end{bmatrix}$ is $\vv{A}^{-1} = \begin{bmatrix} 1/3& 0 \\ 0 & 1/5\end{bmatrix}$ as $\vv{AA}^{-1} = \vv{I}$

\bigskip


```{r}
mm <- matrix(c(1,1,1,2,2,1,0,1,0), ncol=3)
mminverse <- solve(mm)
```

The inverse of $\vv{B} = `r write_matex2(mm)`$ is $\vv{B}^{-1}=`r write_matex2(mminverse)`$
\bigskip

$\vv{C} = `r write_matex2(matrix(c(2,1,0,0,1,0,1,1,0), byrow=F, ncol=3))`$ is not invertible: $\vv{C}^{-1}$ does not exist 

## Span, linear independence

- **linear combination** is just sum of some scaled vectors
  - $\lambda_1\cdot \vv{a}_1 + \lambda_2 \cdot \vv{a}_2+\ldots + \lambda_n\vv{a}_n$, $\vv{a}_i \in R^m$ for $i=1,\ldots,n$ 
  - $\vv{a}_{i}$ are vectors (of the same length) and $\lambda_i$ are the scalars 
  
  
- **span** is the set of all possible linear combination $$\text{span}( \{\vv{a}_{1}, \vv{a}_2, \ldots,\vv{a}_n\}) = \left\{\sum_{i=1}^n \lambda_i \vv{a}_i | \lambda_i \in R, i= 1,\ldots,n\right \}$$
  - what is the span of $\{[1,0]^T, [0,1]^T\}$? 
  - how about $\text{span}( \{[2,1]^T, [0,1]^T\})$? 
  - how about $\text{span}( \{[2,1]^T, [4,2]^T\})$?
  - how about $\text{span}(\{[2,1,0]^T, [0,1,0]^T\})$?
    - it is a **subspace** ($xy$ plane) in $R^3$
    

##

$$\vv{a}_1 =[2,1,0]^T, \vv{a}_2=[0,1,0]^T$$ the span is the $xy$ plane, and it is a subspace (*check the definition of subspace!*)

```{r, out.width="50%"}
x0 <- c(0,0)
y0 <- c(0,0)
z0 <- c(0,0)
x1 <- c(2,0)
y1 <- c(1,1)
z1 <- c(0,0)
cols <- c("#1B9E77", "#D95F02", "#7570B3", "#E7298A")

arrows3D(x0, y0, z0, x1, y1, z1, col = cols,
         lwd = 2, d = 3, 
         main = "", bty ="g", ticktype = "detailed", xlim= c(-0.5, 3), ylim= c(0, 2), zlim= c(0, 2))
# Add starting point of arrow
# points3D(x0, add = TRUE, col="darkred", 
#           colkey = FALSE, pch = 19, cex = 1)
# Add labels to the arrows
text3D(x1, y1, z1, c("a1", "a2"),
        col = cols, add=TRUE, colkey = FALSE)
```

## 

- **linear independence**: $\{\vv{a}_1, \ldots, \vv{a}_n\}$ are linear independent if there exist no $\lambda_1, \ldots, \lambda_n$ (except all being 0) such that $$\lambda_1\cdot \vv{a}_1 + \lambda_2 \cdot \vv{a}_2+\ldots + \lambda_n\vv{a}_n=\vv{0}$$

  - how about $\{[2,3]^T, [4,6]^T\}$ ?
  - are $\{[1,0]^T, [0,1]^T\}$ LI?
  - essentially a way to tell whether there is any *redundant* vectors in the set
  
  
- **rank** of $\vv{A} = [\vv{a}_1, \ldots, \vv{a}_n]$: the maximum number of linearly independent column vectors 
  - the span of the column vectors is called *column space*
  - *rank* also called the *dimension* of the column space 
  - *dimension*: the minimum $\#$ of the vectors needed to span a space
    - what is the *dimension* for $R^3$?


## Example

The column vectors of the matrix

$$[\vv{a}_1, \vv{a}_2, \vv{a}_3]= \begin{bmatrix} 2 &0 & 1 \\
1&1& 1 \\
0 &0&0
\end{bmatrix}$$

are not linearly independent, as 
$$\lambda_1 \vv{a}_1 +\lambda_2\vv{a}_2+\lambda_3\vv{a}_3 = \vv{0}$$

holds for $\lambda_1=\lambda_2=1, \lambda_3=-2$. In other words, one of them is redundant. And $\text{rank}(\vv{A}) = 2$

  

## Matrix vector multiplication
\setbeamercovered{invisible}
$$\vv{A}\vv{x} = \begin{bmatrix}
\vert & \vert &  & \vert\\
\vv{a}_1 & \vv{a}_2 & \ldots & \vv{a}_{n} \\
\vert & \vert & & \vert 
\end{bmatrix} \begin{bmatrix}
x_{1}\\
x_{2}  \\
\vdots  \\
x_{n} 
\end{bmatrix} =  x_1 \begin{bmatrix}
\vert \\
\vv{a}_{1}  \\
\vert 
\end{bmatrix} + x_2 \begin{bmatrix}
\vert \\
\vv{a}_{2}  \\
\vert 
\end{bmatrix}+\ldots + x_n \begin{bmatrix}
\vert \\
\vv{a}_{n}  \\
\vert 
\end{bmatrix}$$

- another view of the multiplication
- *linear combination* of the column vectors of $\vv{A}$
  - $x_1\cdot \vv{a}_1 + x_2 \cdot \vv{a}_2+\ldots + x_n\vv{a}_n$
  - $\vv{a}_{i}$ are the column vectors and $x$ are the scalars 
- so ... $\vv{Ax} = \vv{y}$ essentially solves for what ?
\pause 
  - $\vv{y}$ is in the column space of $\vv{A}$ or not ...
  - if not, then there is no solution
  - if yes, there will be some solution(s)? unique solution or ?



## Matrix vector multiplication (some interpretations)

So $\vv{A}\vv{x}$ is a linear transformation: $R^n\rightarrow R^m$

```{r}
angle = pi/4
R = matrix(c(cos(angle), -1*sin(angle), sin(angle), cos(angle)), byrow = T, nrow = 2)
```

- rotation: rotate $\vv{x}$ anti-clockwise by $\theta$
  $$R\vv{x} = \begin{bmatrix} cos\theta & -sin\theta \\
  sin\theta & cos\theta\end{bmatrix} \begin{bmatrix} x_1\\ x_2\end{bmatrix} = `r write_matex2(R)` \begin{bmatrix} x_1\\ x_2\end{bmatrix}$$ say $\theta = \pi/4$

```{r, out.width='50%'}
xlim <- c(-3,3)
ylim <- c(-1,3)
par(mar=c(3,3,3,0.1)+.1)
plot(xlim, ylim, type="n", xlab="", ylab="", cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5, asp = 1)
grid()
# define some vectors
a=c(2,1)
angle = pi/4
R = matrix(c(cos(angle), -1*sin(angle), sin(angle), cos(angle)), byrow = T, nrow = 2)
aprime = t(R%*%(a))
aprimep = t(R%*%t(aprime))
# plot the vectors
vectors(a, labels="a", pos.lab=4, cex.lab = 3, col="red")
vectors(aprime, labels="Ra", pos.lab=4, cex.lab = 3)
vectors(aprimep, labels="RRa", pos.lab=4, cex.lab = 3)
```

##

  - $\vv{R}$ is a rotation (orthogonal) matrix if $\vv{R}^T = \vv{R}^{-1}$ (what does it imply?)
  - $\vv{R}^T\vv{R} = \vv{R}\vv{R}^T =\vv{I}$
  \pause 
    - preserves length $(\vv{R}\vv{x})^T(\vv{R}\vv{x}) = \vv{x}^T\vv{R}^T\vv{R}\vv{x} = \vv{x}^T\vv{x}$
  - rotation transform is alwasy invertible: inverse means rotating back!
  - mathematics is always so reasonable :-)

##

Projection: project $\vv{x}\in R^n$ to $\vv{a}\in R^n$ \begin{align*}P(\vv{x}, \vv{a}) &= \|\vv{x}\|cos \theta * \frac{\vv{a}}{\|\vv{a}\|} =  \frac{\vv{a}^T\vv{x}}{\vv{a}^T\vv{a}}  \vv{a} \\
&= \frac{\vv{a}\cdot \vv{a}^T\vv{x}}{\vv{a}^T\vv{a}} = \frac{\vv{a}\vv{a}^T}{\vv{a}^{T}\vv{a}}\vv{x}=\vv{Px}\end{align*}

$$\vv{P} = \frac{\vv{a}\vv{a}^T}{\vv{a}^{T}\vv{a}}$$
  
  - $\vv{P}$ is $n$ by $n$ square matrix
  - $\vv{P}$ is a projection matrix: $\vv{Px}$ returns the projection
  - what if we project it again (and again ...) ? $\vv{P}\ldots\vv{P}\vv{P}\vv{x}$
  \pause 
    - it remains unchanged, $\vv{P}\ldots\vv{P}\vv{P}\vv{x} = \vv{P}\vv{x}$
    - or $\vv{P}\vv{P} = \vv{P}$
   $$\frac{\vv{a}\vv{a}^T}{\vv{a}^{T}\vv{a}}\frac{\vv{a}\vv{a}^T}{\vv{a}^{T}\vv{a}}= \frac{\vv{aa}^T\vv{aa}^T}{(\vv{a}^T\vv{a})^2}= \frac{\vv{a}\vv{a}^T}{\vv{a}^{T}\vv{a}}$$
  - some projection cannot be reversed: some $\vv{P}$ is not invertible !
  - mathematics always makes sense :-)
  
  
\section{Calculus}
\only<article>{Our revision on calculus will be brief. We will only review the concept of partial derivative, gradient, and hessian matrix. You should try to convince yourself all the results listed here if you can.}

## Calculus

- univariate derivative
\bigskip

- partial derivative 
\bigskip

- gradient

\bigskip

- hessian matrix

\bigskip

- quadratic form and its derivative

\bigskip

- Lagrange multiplier



## Derivative 

Given function $f(x): R\rightarrow R$, the derivative is defined as

$$\frac{df}{dx} \equiv Df(x) \equiv \lim_{\Delta x \rightarrow 0} \frac{f(x+\Delta x) - f(x)}{\Delta x} $$

- the growth rate at or near $x$
- the derivative points in the direction of steepest ascent of f
- when $Df(x) =0$, $f$ doesn't grow or decline near $x$, or *stationary*
  - either maximum (negative second derivative) or minimum (positive second derivative)
  - saddle point
  - for optimisation, just find $Df(x) = 0$


## Example

$f(x) = -x^2$ what are the derivatives at -2 and 2?

- $Df(x)  = -2x$; 
- at $x=-2$, $Df(-2)=4 >0$ points to the right (ascent direction)
- at $x=2$, $Df(2) =-4 <0$ points to the left (ascent direction)
- at $x=0$, $Df(0) = 0$, it's maximum as $D^2f(0) =-2<0$ 
\bigskip

```{r, out.width= "60%"}
par(mar=c(3,3,3,0.1)+.1)
curve(-x^2, -5, 5, ylab = "f", ylim= c(-25, 5), cex.lab=2, cex.axis=2, cex.main=2, cex.sub=2)
curve(-4*x+4, -2, 5, add=T, col="red")
curve(4*x+4, -5, 2, add=T, col="red")
points(-2, y = -4, type = "p", pch=16)
points(2, y = -4, type = "p", pch=16)
text(-2.2, -2, expression(paste( "Df(-2) = 4")), cex = 3)
text(2.8, -2, expression(paste( "Df(2) = -4")), cex=3)
```


## Partial derivative and gradient

Given function $f(\vv{x}): R^n\rightarrow R$, partial derivative w.r.t. $x_i$ is

$$\frac{\partial f}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_i+h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h} $$

The vector 

$$\nabla_{\vv{x}}f=\text{grad} f = \frac{df}{d\vv{x}} = \left [\frac{\partial f(\vv{x})}{\partial x_1}, \frac{f(\vv{x})}{\partial x_2}, \ldots, \frac{f(\vv{x})}{\partial x_n}\right ]$$

- partial derivative measure the growth rate at axis aligned directions 
- similarly, the gradient points to the greatest ascent direction 


## Example 

Find the gradient of $f(\vv{x}) = f(x_1, x_2) = x_1^2 +x_2^2$

$$\nabla_{\vv{x}}f = \left [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right ] = [2x_1, 2x_2]$$

- gradient is a vector field: at any input point $[x_1, x_2]$, it gives a direction vector $[2x_1, 2x_2]$

```{r, out.width="60%"}
M <- mesh(seq(-10, 10, by=0.1),seq(-10, 10, by = 0.1))
z = M$x ^2 + M$y^2
surf3D(x=M$x, y=M$y, z= z, colkey = TRUE, 
       box = TRUE, bty = "b", phi = 20, theta = 120, colvar = z, xlab="x1", ylab="x2", zlab="f")
```

##

- at $\vv{x} = [1,1]$, the gradient vector is $[2,2]$, pointing to steepest ascent direction 
- at $\vv{x} = [1,0]$, the gradient vector is $[2,0]$
- at $\vv{x} = [0,0]$, the gradient vanishes


\begin{figure}
    \centering
    \includegraphics[width = 0.6\textwidth]{./gradvecfield.eps}
 % \caption{Awesome figure}
\end{figure}

```{r}
library(pracma)
```

## Quadratic form $x^TAx$

Another view: $f(\vv{x}) = x_1^2 +x_2^2= \vv{x}^T \vv{A} \vv{x}$, where $\vv{A}$ is $\vv{I}$ (*verify this!*)

  - $\vv{x}^T\vv{Ax}$ is called quadratic form
  - generalisation of quadratic function $f(x) = ax^2 = xax$
  - the gradient is $\nabla_{\vv{x}} f = 2\vv{A}\vv{x}$ (when $A$ is symmetric)
  - the hessian matrix is $\nabla_{\vv{x}}^2 f = 2\vv{A}$
    - $f$ has a maximum if $\vv{A}$ is negative definite ($a$ is negative) 
    - $f$ has a minimum if $\vv{A}$ is positive definite ($a$ is positive)
  - positive definite (P.D.) matrix $\vv{A}$: $\vv{x}^T\vv{Ax} >0$ for all $x\in R^m$
    - $\vv{I}$ is P.D., why ?
    - $f$ has a minimum 
  - negative definite (N.D.) matrix $\vv{A}$: $\vv{x}^T\vv{Ax} <0$ for all $x\in R^m$

    
    
## Example: connecting the dots

*Given a bunch of numbers $\vv{x} = [3, 4, 3.5, 5, 6, 5.5]$, to summarise the data set, sample mean $$\mu(\vv{x}) = \frac{1}{N} \sum_{i=1}^N {x_i}$$ is often used; why ? *

##

- $(\mu -x_i)^2$ meansures the distance between $x_i$ and $\mu$
- $\sum_{i=1}^{N} (\mu -x_i)^2$ measures the total distance or cost
- note that $\mu\begin{bmatrix}1\\ \vdots \\1 \end{bmatrix} - \begin{bmatrix}x_1 \\\vdots \\x_n\end{bmatrix} = \begin{bmatrix}\mu-x_1 \\\vdots \\\mu-x_n\end{bmatrix} \equiv \vv{e}$
\begin{align*} 
\text{Let } f(\mu) &= \sum_{i=1}^{N} (\mu -x_i)^2 = (\mu \vv{1}- \vv{x})^{T}(\mu \vv{1}- \vv{x})= \vv{e}^T\vv{e} = \vv{e}^T\vv{I}\vv{e} \\
\frac{df}{d\mu} &=  \underbrace{\frac{\partial f}{\partial \vv{e}}\frac{d\vv{e}}{d\mu}}_{\text{chain rule}}= 2\vv{e}^T\vv{1} = 2(\mu\vv{1} - \vv{x})^T \vv{1}
\end{align*}

\bigskip

set the derivative to 0
\begin{align*} 
\mu\vv{1}^T\vv{1} -\vv{x}^T \vv{1}= 0 \Rightarrow  \mu = \frac{\vv{1}^T\vv{x}}{\vv{1}^T\vv{1}} = \frac{\sum_{i}^N x_i}{N}
\end{align*}

- can you tell it is actually a projection ?

##

- note that the projection of $\vv{x}$  on $\vv{1}$ is $$P(\vv{x}, \vv{1}) = \frac{\vv{1}^T\vv{x}}{\vv{1}^T\vv{1}} \vv{1} = \mu \vv{1}$$

- this is a ML model actually (specific case) 
- for this simple example, we have used 
  - linear algebra
  - vector calculus
- how about probability theory ?
  
  
  
# Probability theory
  
## Probability theory

- Random variable
\bigskip
- Probability distribution
\bigskip
- Probability mass function and density function
\bigskip
- Probability rules
\bigskip
- Expectation, variance, covariance
\bigskip
- Conditional expectation

## Random variable and probability distribution

**Random variable** $X$ associates with a **probability distribution** $P(X)$

  - formally, a r.v. is a mapping from sample space $\Omega$ to a target space $\mathcal{T}$
  - e.g. toss a fair coin twice, r.v. $X$ is the number of heads turned up
    - the sample space is $\Omega =\{HH, TT, HT, TH\}$
    - target space is $T =\{0,1,2\}$
    - the probability distribution is 
$$P(X) = \begin{cases} 0.25 & X=0 \\
0.5 & X=1 \\
0.25 & X=2 \end{cases}$$    
  
  - the distribution $P$ must satisfy
  $$P(X=x) >0, \text{ and } \sum_{x\in T} P(X=x) =1$$
  
  

## Random variable - discrete r.v.
  
If r.v. $X$'s target space $\mathcal{T}$ is discrete

  - $X$ is called **discrete random variable**
  - the probability distribution $P$ is called **probability mass function** (p.m.f.)
  - and $$0\leq P(X=x) \leq 1, \text{ and } \sum_{x\in T} P(X=x) =1$$
  
  
  
## Example - discrete r.v.
  
  **Bernoulli distribution**
  Tossing a coin , $\mathcal{T} = {1, 0}$ (1 is $H$, 0 is $T$), $$P(X=1) = p , P(X=0) = 1-p, 0\leq p\leq 1$$
  
  **Binomial distribution**
  Tossing a coin $N$ times, the r.v. $X$ is the number of head shows up
  $$P(X=k) = \binom{N}{k} \cdot p^k(1-p)^{N-k}$$ 
  
  - what's the relationship between Binomial and Bernoulli?
    
    \bigskip
    
  **Multinoulli distribution**
  Throw a fair 6-facet die, $\mathcal{T} = {1, 2,\ldots, 6}$, the distribution is 
  $$P(X=i) = 1/6$$  
  
  *Verify the above $P$s satisfy the conditions of p.m.f.*
  
  
## Random variable - continuous r.v.

If r.v. $X$'s target space $\mathcal{T}$ is continuous
  
  - $X$ is called **continuous random variable **
  - the probability distribution $p$ is called **probability density function** (p.d.f.)
  - and satisfies $$p(x) \geq 0, \text{ and } \int_{x\in T} p(x) dx = 1$$
  - pdf is not probability as $p(x)$ can be greater 1; 
  - for $\forall x$ $P(X=x) =0$  
  - calculate probability over an interval: e.g. $$P(X \in [a,b]) = \int_{a}^b p(x) dx$$
  

  
## Example - continuous r.v.
  
  **Uniform distribution**
  $\mathcal{T} = [0,1]$, $X$ has equal chance to take any value between 0 and 1; the pdf is 
$$p(x) = \begin{cases} 1 & x\in [0,1] \\
0 & \text{otherwise} \end{cases} $$

```{r, out.width="50%"}
a=0
b=1
par(mar=c(4,5,4,0.1)+.1)
curve(dunif(x, min = a, max = b), 
      from = -0.5, to = 1.5, 
      n = 100000, 
      col = "darkblue", 
      lwd = 2, 
      yaxt = "n",
      ylab = 'pdf', cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)
```
Easy to verify $\int_0^1 p(x)dx = \int_0^1 dx =1$
\bigskip

What's the probability that $0<X<0.5$ ? 

## Example - continuous r.v.

**Gaussian distribution**
$\mathcal{T} = R$, or $X \in R$ the pdf is

$$p(x) = \normal{x; \mu}{\sigma^2}=\frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$
 $(\frac{x-\mu}{\sigma})^2$ is a distance measure: how far $x$ is away from $\mu$ (measured by $\sigma$ as a unit)

```{r, out.width="65%"}
a=0
b=1
par(mar=c(3,5,3,0.1)+.1)
curve(dnorm(x, mean = 0, sd = 0.5), 
      from = -11, to = 11.5, 
      n = 100000, 
      col = "darkblue", 
      lwd = 2, 
      yaxt = "n",
      xlab="",
      ylab = 'pdf', cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)

curve(dnorm(x, mean = 0, sd = 1), 
      from = -10, to = 10, 
      n = 100000, 
      col = "red", 
      lwd = 2, 
      yaxt = "n",
      add= T,
      ylab = 'pdf', cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)

curve(dnorm(x, mean = 0, sd = 3), 
      from = -10, to = 10, 
      n = 100000, 
      col = "green", 
      lwd = 2, 
      yaxt = "n",
      add= T,
      ylab = 'pdf', cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)

curve(dnorm(x, mean = 5, sd = 1), 
      from = -10, to = 10, 
      n = 100000, 
      col = "darkgreen", 
      lwd = 2, 
      yaxt = "n",
      add= T,
      ylab = 'pdf', cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)

curve(dnorm(x, mean = -5, sd = 1), 
      from = -10, to = 10, 
      n = 100000, 
      col = "gold", 
      lwd = 2, 
      yaxt = "n",
      add= T,
      ylab = 'pdf', cex.lab=2.5, cex.axis=2.5, cex.main=2.5, cex.sub=2.5)

labels <- c(expression(paste(mu, "=0, ", sigma, "=0.5")), expression(paste(mu, "=0, ", sigma, "=1")), expression(paste(mu, "=0, ", sigma, "=3")), expression(paste(mu, "=5, ", sigma, "=1")), expression(paste(mu, "=-5, ", sigma, "=1")))

legend("topright",  title="Gaussians",
  labels, lwd=2, lty=c(1, 1, 1, 1, 1), col=c("darkblue", "red", "green", "darkgreen", "gold"
                                             , cex=3, pt.cex = 3))
```

## Question 

Calculate quickly: $$\int_{-\infty}^{\infty} e^{-\frac{1}{2}x^2}dx = ?$$ 

\bigskip

For $X\sim \normal{\mu}{\sigma}$, what is $P(X<\mu)=$?


## Joint distribution

- r.v. $\vv{X} = [X_1, X_2, \ldots, X_n]^T$ can be multidimensional (each $X_i$ is r.v.)
  - essentially a *random vector*
  
  \bigskip 
- Still satisfies the same requirements
  $$\forall \vv{x}, 0<P(\vv{X}=\vv{x}) <1,\; \sum_{x_1}\sum_{x_2}\ldots\sum_{x_n} P(\vv{X} =[x_1, x_2, \ldots, x_n]) =1$$
or $$\forall \vv{x}, p(\vv{X}=\vv{x}) >0,\; \int\int\ldots\int p(\vv{X} =\vv{x})d{x_1}d{x_2\ldots dx_{n}} =1$$
  \bigskip 
- for bivariate case, i.e. $n=2$, $X_1, X_2$ are **independent** if $P(\vv{X}) = P(X_1)P(X_2)$ (e.g. rolling two dice independently)

## Example: discrete joint distribution

The joint distribution of $X$ snow or not, $Y\in$ \{\text{spring}, \text{summer}, \text{autumn}, \text{winter}\} represents the season that $x$ belongs to :
\bigskip 
\begin{table}\centering
\begin{tabular}{ l | c | c | c | c}
   \centering                    
   & $y=\text{Spring}$ & $y=\text{Summer}$ &$y=\text{Autumn}$ & $y=\text{winter}$\\ 
   \hline
  $x= F$ & `r 1/4* 0.2` & `r 1/4* 1` & `r 1/4* 0.3`& `r 1/4* 0`\\
	\hline 
  $x= T$ & `r 1/4* 0.8` & `r 1/4* 0` & `r 1/4* 0.7`& `r 1/4* 1`\\ 
\end{tabular}
\end{table}
\bigskip 
It is easy to verify that    
\[\sum_x\sum_y p(x, y) = 1\]
   
## Example: continuous joint distribution


If $X,Y$'s joint p.d.f is 
$$p(x,y) = \frac{1}{2\pi\sigma_x\sigma_y} e^{-\frac{1}{2}[(\frac{x-\mu_x}{\sigma_x})^2 +(\frac{y-\mu_y}{\sigma_y})^2] }$$

$X,Y$ are bivariate Gaussian distributed 

\bigskip
X,Y are *independent* for this case, why ? 



## Probability rules 

There are only two probability rules (integration for continuous r.v.):
\begin{enumerate}
	\item product rule: \[ p(x, y) = p (y|x)p(x) = p(x|y)p(y)\]
	\item sum rule (marginalisation): \[ p (x) = \sum_y p(x, y),\; p(y) = \sum_x p(x, y)\]
\end{enumerate}




## Conditional probability

Conditional probability distribution (by product rule): \[p (x| y) = \frac{p(x, y)}{p(y)}\]
  
  - probability distribution of $x$ conditional on the value of $y$
  
  \bigskip
  
\begin{table}\centering
\begin{tabular}{ l | c | c | c | c}
   \centering                    
   & $y=\text{Spring}$ & $y=\text{Summer}$ &$y=\text{Autumn}$ & $y=\text{winter}$\\ 
   \hline
  $x= F$ & `r 1/4* 0.2` & `r 1/4* 1` & `r 1/4* 0.3`& `r 1/4* 0`\\
	\hline 
  $x= T$ & `r 1/4* 0.8` & `r 1/4* 0` & `r 1/4* 0.7`& `r 1/4* 1`\\ 
\end{tabular}
\end{table}  

\bigskip
- $P(Y= \text{Spring})$ ? use sum rule
  $P(Y = \text{Spring}) = \sum_{x=\{T,F\}} P(X=T,Y=\text{Spring}) =0.05+0.2=0.25$
  
- $P(X=T | Y= \text{Spring})$ ?
  $P(x=T | y = \text{Spring}) = \frac{P(x=T, y=\text{Spring})}{P(y=\text{Spring})}=\frac{0.2}{0.25}=`r 0.2/0.25`$
<!-- The total probability rule also implies marginalization rule:\[ p (x) = \sum_y p(x, y) \] -->

<!-- Baye's theorem is then just a combination of the above three rules: \[ p (x| y) = \frac{p(y|x) p(x)}{\sum_x p(y|x) p(x)}\] -->



## Expectation and variance

**Expection** of a r.v. is defined as 
$$\E{g(X)} = \sum_x g(x) P(x) \text{ or } \E{g(X)} = \int g(x) P(x)dx$$
  
- $\E{a} = a$, $a$ is a constant (not r.v.)
- $\E{\E{X}} = \E{X}$
- $\E{aX +bY}  = a\E{X} + b\E{Y}$: linearity

\bigskip

**Variance** of a r.v. is defined as
$$\Var{g(X)} = \E{(g(X)-\E{g(X)})^2}$$ 
  
- $\Var{X} = \E{X^2} - \E{X}^2$
- $\Var{aX} = a^2\Var{X}$

\bigskip

*Prove them or convince yourself !*
\only<article>{A very useful identity that links expectation and variance together is 
\[\Var{x} = \E{x^2} - \E{x}^2 , \] which can be proved as follows:
\[ \Var{x} = \E{x^2 -2\E{x} x + \E{x}^2} = \E{x^2} -2\E{x}^2 + \E{x}^2 = \E{x^2} - \E{x}^2; \]
 the second equality holds because $\E{x}=\mu$  is a constant. Note that $\E{x^2}$ is called the \textit{second moment} of r.v. $x$.}


## Example

$X$ is a Bernoulli r.v. with parameter $p=0.5$; what is $\E{X}$?

- $\E{X} = 1\times P(X=1) + 0\times P(X=0) = p =0.5$;

\bigskip

$Y$ is a Binomial r.v. with $N=10, p=0.5$, what is $\E{Y}$?

- $Y= \sum_{i=1}^{N} X = N\times X$
- $\E{Y} = \E{N\times X} = N\times \E{X} = N\times p = 5$
- interpretation: you expect to see 5 successes out of 10 (on average the result is 5 if you repeat the experiment a lot of times)





## General advice on reading maths

\begin{tcolorbox}[colback=red!5!white]
  {\large ``The fish trap exists because of the fish. Once you've gotten the fish you can forget the trap. The rabbit snare exists because of the rabbit. Once you've gotten the rabbit, you can forget the snare. Words exist because of meaning. Once you've gotten the meaning, you can forget the words. Where can I find a man who has forgotten words so I can talk with him?''}
  \hspace*\fill{\small--- Zhuang Zhou}
\end{tcolorbox}

- maths intuition is more important than equations

\bigskip

- think first then verify your beliefs by reading (different) text books or code it up and check 

\bigskip

- stand higher and look at a greater picture
  - accepting intermediate results without fully understanding is totally cool
